---
title: FAQ
description: Quick answers to common questions.
---

### How do I authenticate the websocket?

Add tokens to the URL via `buildUrl()` on the client or inject headers in your server adapter before calling `createVoiceSession`/`createVoiceDurableObject`.

### Can I use custom STT/TTS?

Yes. Implement `TranscriptionProvider`/`SpeechProvider` from `@usevoiceai/server` and plug them into the session factory. Both return async-iterable streams (`TranscriptStream` and `SpeechStream`) if you want to consume events/chunks directly.

### How do I auto-detect end of speech?

Pass `speechEndDetection: { mode: "auto" }` to the client; ensure your transcription provider emits `speech-end` hints (enable VAD/endpointing via `speechEndDetection.options`). Otherwise, use manual `end`.

### Can I play audio my own way?

Consume `speechStream` directly as an async iterable of PCM chunks. Feed it into Web Audio, MediaSource, or a custom encoder.

### Multiple sessions per user?

Each WebSocket maps to one `VoiceSession`. Open multiple sockets if you need parallel sessions or namespace by `userId`.

### Does it work with SSR or tests?

Yes. Provide `WebSocketImpl` and `mediaDevices` mocks for tests; real recording still requires a browser environment.
