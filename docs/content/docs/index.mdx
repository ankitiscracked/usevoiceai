---
title: Introduction
---

`usevoiceai` is the Typescript toolkit to build ambitious voice AI apps. You can think of it as the voice native version of [AI SDK](https://sdk.vercel.ai/). It heavily focuses on time to value and great DX.

## Why should you use it?

- You want to integrate voice interactivity into your app within minutes
- You don't know much about [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API)
- You don't know much about the [Web Audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API)
- You want to use the latest voice models but you don't want vendor lock-in
- You care about ergonomics and great DX

## Minimum setup

`usevoiceai` has two main parts for the minimum setup required to get up and running.

- **Client**: `useVoice` and `useSpeech` hooks which are the main interfaces for capturing your voice and playing the response speech on your web browser.
- **Server**: Framework agnostic voice capture and playback session, adapters for transports such a Durable Objects, Node WebSockets, etc., and default providers for transcription, speech generation with a pluggable Agent interface.

Here's the minimum that required to get you up and running.

```ts
import { useVoice, useSpeech } from "usevoiceai";

const { startRecording, stopRecording, transcript, speechStream } = useVoice();
const { stop } = useSpeech({ speechStream });
```

```ts
import { VoiceSessionDO } from "usevoiceai";

const VoiceSessionDO = createVoiceDurableObject<Env>({
  transcription: (env) => deepgram("nova-3", { apiKey: env.DEEPGRAM_API_KEY }),
  agent: (env) => new MockAgentProcessor(env),
  speech: (env) => cartesia("sonic-3", { apiKey: env.CARTESIA_API_KEY }),
});

export default {
  async fetch(request: Request, env: Env) {
    const url = new URL(request.url);

    if (url.pathname === "/voice-command/ws") {
      const userId = url.searchParams.get("userId") ?? "demo-user";
      const id = env.VOICE_SESSION.newUniqueId();
      const stub = env.VOICE_SESSION.get(id);
      return stub.fetch(new Request(request, { headers }));
    }
    return new Response("Not found", { status: 404 });
  },
};
```

## Why it exists?

Voice models have made incredible leaps in recent years but the infra to integrate them in real apps is still lacking. Model providers gladly provide APIs and SDKs and they are great but the ecosystem is fragmented. Providers have different contracts and quirks. Often, choosing one means leaving other. Or worse painfully glueing different parts together to make it work.

Some of the technical problems it solves:

- Models are already great. This package just augments them on the infra layer
- Provides unified interfaces to work with transcription providers and speech generation providers
- Handles the intricacies of socket connections and session management
- Handles the complex pipeline of STT -> your custom agent -> TTS
- Handles Automatic Speech Recognition based turn processing
- Handles the intricacies of audio recording on web browsers
- Provides simple intuitive APIs for good DX
- Provides convenience utilities for both client and server

## Pluggable architecture

`usevoiceai` is designed to be **pluggable** and **composable**.

You can use any provider for transcription and speech generation you want. We have a few providers out of the box (more coming soon):

- **Speech-To-Text**: [Deepgram](https://deepgram.com/)
- **Text-To-Speech**: [Cartesia](https://cartesia.ai/), [Hume](https://hume.ai/)

You can also create your own provider by implementing the `TranscriptionProvider` and `SpeechProvider` interfaces.

<Callout type="info">
  See [Providers](/docs/providers) for more details.
</Callout>

On the transport layer, we have a few adapters for popular server runtimes:

- **Cloudflare Durable Object**: `createVoiceDurableObject`
- **Node WebSocket**: `registerNodeWebSocketServer`

<Callout type="info">
  See [Quickstart (Server)](/docs/quickstart-server) for more details.
</Callout>

## Focus on proving great DX

I've meticulously obsessed over the DX of this project. I don't shy from accepting that the design of AI SDK has been a huge influence.
The client hooks are intuitive and expose as much as needed to get you up and running quickly. The server is framework agnostic and provides a default helper implementations for popular STT and TTS providers. The providers are pluggable and you can provide your own implementations. For example, if you prefer to use local voice models.
